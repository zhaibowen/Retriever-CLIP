CLIPConfig_medium()
	batch_size: 24
	beta1: 0.9
	beta2: 0.95
	gpu_num: 3
	grad_clip: 1.0
	gradient_accumulation_steps: 2
	hidden_size: 768
	img_hidden_size: 2048
	img_num_head: 32
	img_size: 336
	learning_rate: 0.0006
	num_epoch: 12
	num_heads: 12
	num_layers: 12
	res_channels: [64, 256, 512, 1024, 2048]
	res_layers: [3, 4, 6, 3]
	sequence_length: 25
	spacial_dim: 11
	vision: resnet50_v15
	vocab_size: 32000
	weight_decay: 0.1

CLIP(
  (image_encoder): ResNetV15(
    (conv1): Conv_Batch_Active(
      (block): Sequential(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (conv2): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (1): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Conv_Batch_Active(
          (block): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (2): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (3): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
    )
    (conv3): Sequential(
      (0): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Conv_Batch_Active(
          (block): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (1): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (2): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (3): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
    )
    (conv4): Sequential(
      (0): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Conv_Batch_Active(
          (block): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (1): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (2): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (3): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (4): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (5): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
    )
    (conv5): Sequential(
      (0): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Conv_Batch_Active(
          (block): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Identity()
          )
        )
      )
      (1): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
      (2): Bottleneck(
        (block): Sequential(
          (0): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (1): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (2): Conv_Batch_Active(
            (block): Sequential(
              (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Identity()
            )
          )
        )
        (shortcut): Identity()
      )
    )
  )
  (img_attn): CrossAttention(
    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
    (o_proj): Linear(in_features=2048, out_features=768, bias=False)
  )
  (text_encoder): Retriever(
    (token_embedding): Embedding(32000, 768)
    (layers): ModuleList(
      (0-11): 12 x DecoderLayer(
        (ln_1): RMSNorm()
        (attn): Attention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): RotaryEmbedding()
        )
        (ln_2): RMSNorm()
        (mlp): MLP(
          (gate_proj): Linear(in_features=768, out_features=2058, bias=False)
          (up_proj): Linear(in_features=768, out_features=2058, bias=False)
          (down_proj): Linear(in_features=2058, out_features=768, bias=False)
        )
      )
    )
    (norm): RMSNorm()
  )
)

total_params: 148.31M, text_params: 109.81M, image_params: 23.51M, other_params: 15.00M
epoch: 0, lr: 0.0006000, device: cuda:0
step 100, loss 0.417, consume 158.19s
step 200, loss 0.268, consume 137.60s
step 300, loss 0.243, consume 137.38s
step 400, loss 0.222, consume 138.63s
step 500, loss 0.223, consume 138.44s
step 600, loss 0.218, consume 139.16s
step 700, loss 0.207, consume 135.87s
step 800, loss 0.206, consume 137.12s
    valid loss: 0.363, consume: 69.945s
epoch: 0, consume: 1224.250s
epoch: 1, lr: 0.0006000, device: cuda:0
step 100, loss 0.185, consume 152.59s
step 200, loss 0.183, consume 135.70s
step 300, loss 0.188, consume 138.39s
step 400, loss 0.182, consume 129.83s
step 500, loss 0.173, consume 134.02s
step 600, loss 0.185, consume 137.44s
step 700, loss 0.179, consume 136.36s
step 800, loss 0.173, consume 137.29s
    valid loss: 0.330, consume: 69.431s
epoch: 1, consume: 1201.345s
epoch: 2, lr: 0.0006000, device: cuda:0
step 100, loss 0.161, consume 152.61s
step 200, loss 0.164, consume 134.77s
step 300, loss 0.164, consume 131.48s
step 400, loss 0.161, consume 129.33s
step 500, loss 0.158, consume 135.36s
step 600, loss 0.163, consume 137.10s
step 700, loss 0.166, consume 136.13s
step 800, loss 0.159, consume 139.50s
    valid loss: 0.335, consume: 69.623s
epoch: 2, consume: 1194.707s
epoch: 3, lr: 0.0006000, device: cuda:0
step 100, loss 0.151, consume 152.07s
step 200, loss 0.150, consume 135.67s
step 300, loss 0.150, consume 136.54s
step 400, loss 0.150, consume 135.76s
step 500, loss 0.144, consume 136.52s
step 600, loss 0.151, consume 138.43s
step 700, loss 0.150, consume 134.68s
step 800, loss 0.152, consume 136.00s
    valid loss: 0.300, consume: 69.984s
epoch: 3, consume: 1205.501s
epoch: 4, lr: 0.0000600, device: cuda:0
step 100, loss 0.122, consume 152.32s
step 200, loss 0.110, consume 136.25s
step 300, loss 0.108, consume 137.26s
step 400, loss 0.103, consume 138.08s
step 500, loss 0.101, consume 136.36s
step 600, loss 0.104, consume 137.86s
step 700, loss 0.095, consume 136.19s
step 800, loss 0.094, consume 138.54s
    valid loss: 0.236, consume: 73.247s
epoch: 4, consume: 1216.137s
epoch: 5, lr: 0.0000600, device: cuda:0
step 100, loss 0.094, consume 158.59s
step 200, loss 0.090, consume 138.09s
step 300, loss 0.094, consume 138.50s
step 400, loss 0.094, consume 137.26s
step 500, loss 0.085, consume 136.65s
step 600, loss 0.090, consume 137.28s
step 700, loss 0.091, consume 135.86s
step 800, loss 0.088, consume 137.07s
    valid loss: 0.228, consume: 70.590s
epoch: 5, consume: 1220.560s
epoch: 6, lr: 0.0000600, device: cuda:0
step 100, loss 0.088, consume 153.37s
step 200, loss 0.085, consume 137.24s
step 300, loss 0.086, consume 135.50s
step 400, loss 0.083, consume 137.37s
step 500, loss 0.086, consume 137.47s
step 600, loss 0.075, consume 136.82s
step 700, loss 0.079, consume 139.10s
step 800, loss 0.080, consume 136.89s
    valid loss: 0.222, consume: 70.156s
epoch: 6, consume: 1214.192s
epoch: 7, lr: 0.0000600, device: cuda:0
step 100, loss 0.077, consume 153.50s
step 200, loss 0.079, consume 138.77s
step 300, loss 0.082, consume 134.61s
step 400, loss 0.080, consume 136.41s
step 500, loss 0.077, consume 139.21s
step 600, loss 0.081, consume 135.95s
step 700, loss 0.079, consume 137.01s
step 800, loss 0.081, consume 135.50s
    valid loss: 0.212, consume: 70.398s
epoch: 7, consume: 1212.616s
epoch: 8, lr: 0.0000060, device: cuda:0
step 100, loss 0.074, consume 153.86s
step 200, loss 0.074, consume 137.33s
step 300, loss 0.074, consume 138.02s
step 400, loss 0.071, consume 135.33s
step 500, loss 0.068, consume 134.54s
step 600, loss 0.075, consume 128.12s
step 700, loss 0.067, consume 132.02s
step 800, loss 0.068, consume 138.22s
    valid loss: 0.210, consume: 71.040s
epoch: 8, consume: 1198.881s
epoch: 9, lr: 0.0000060, device: cuda:0
step 100, loss 0.072, consume 154.06s
step 200, loss 0.074, consume 132.22s
step 300, loss 0.077, consume 129.10s
step 400, loss 0.071, consume 136.53s
step 500, loss 0.072, consume 137.95s
step 600, loss 0.074, consume 136.82s
step 700, loss 0.074, consume 132.63s
step 800, loss 0.075, consume 133.11s
    valid loss: 0.207, consume: 59.331s
epoch: 9, consume: 1181.471s
epoch: 10, lr: 0.0000060, device: cuda:0
step 100, loss 0.071, consume 143.75s
step 200, loss 0.077, consume 129.50s
step 300, loss 0.068, consume 129.77s
step 400, loss 0.073, consume 127.75s
step 500, loss 0.073, consume 127.19s
step 600, loss 0.073, consume 129.77s
step 700, loss 0.073, consume 128.88s
step 800, loss 0.070, consume 128.21s
    valid loss: 0.207, consume: 59.447s
epoch: 10, consume: 1131.706s
